{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "9jy_qc01rt5K"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import ast  # Library for handling literal_eval\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "Preparing the COVID dataset for training...\n",
      "Preparing the Pre-COVID dataset for testing...\n",
      "Training the model on COVID data...\n",
      "Testing the model on Pre-COVID data...\n",
      "Accuracy: 0.66\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.51      0.56     22674\n",
      "           1       0.67      0.78      0.72     28806\n",
      "\n",
      "    accuracy                           0.66     51480\n",
      "   macro avg       0.65      0.64      0.64     51480\n",
      "weighted avg       0.65      0.66      0.65     51480\n",
      "\n",
      "Training the model on Pre-COVID data...\n",
      "Testing the model on COVID data...\n",
      "Accuracy: 0.67\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.56      0.59     12637\n",
      "           1       0.69      0.75      0.72     16682\n",
      "\n",
      "    accuracy                           0.67     29319\n",
      "   macro avg       0.66      0.66      0.66     29319\n",
      "weighted avg       0.67      0.67      0.67     29319\n",
      "\n",
      "\n",
      "COVID dataset evaluation:\n",
      "Training the model...\n",
      "Testing the model...\n",
      "Accuracy: 0.67\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.55      0.59      2508\n",
      "           1       0.69      0.76      0.73      3356\n",
      "\n",
      "    accuracy                           0.67      5864\n",
      "   macro avg       0.66      0.66      0.66      5864\n",
      "weighted avg       0.67      0.67      0.67      5864\n",
      "\n",
      "\n",
      "Pre-COVID dataset evaluation:\n",
      "Training the model...\n",
      "Testing the model...\n",
      "Accuracy: 0.67\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.58      0.61      4479\n",
      "           1       0.70      0.75      0.72      5817\n",
      "\n",
      "    accuracy                           0.67     10296\n",
      "   macro avg       0.67      0.66      0.66     10296\n",
      "weighted avg       0.67      0.67      0.67     10296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def preprocess_df(df):\n",
    "    quantile_001 = df['duration'].quantile(0.01)\n",
    "    quantile_099 = df['duration'].quantile(0.99)\n",
    "    df_filtered = df[(df['duration'] >= quantile_001) & (df['duration'] <= quantile_099) & (df['duration'] > 1)]\n",
    "    return df_filtered.dropna().sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def load_and_preprocess(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return preprocess_df(df)\n",
    "\n",
    "def generate_features_labels(df, threshold=25):\n",
    "    X = df.drop(['duration'], axis=1)\n",
    "    y = (df['duration'] > threshold).astype(int)\n",
    "    return X, y\n",
    "import lightgbm as lgb\n",
    "def train_model(X_train, y_train, model=lgb.LGBMClassifier(random_state=42)):#XGBClassifier(n_estimators=200,n_jobs=10)):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "# Prepare data, then train & test for COVID and Pre-COVID datasets\n",
    "def prepare_train_test_evaluate(df, test_size=0.2, random_state=420):\n",
    "    X, y = generate_features_labels(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Standardize features\n",
    "    X_train_scaled, X_test_scaled = X_train, X_test\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training the model...\")\n",
    "    clf = train_model(X_train_scaled, y_train)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"Testing the model...\")\n",
    "    evaluate_model(clf, X_test_scaled, y_test)\n",
    "    \n",
    "    \n",
    "# Main workflow\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading the datasets...\")\n",
    "    df_covid = load_and_preprocess('merged_covid.csv')\n",
    "    df_pre_covid = load_and_preprocess('merged_before.csv')\n",
    "\n",
    "    print(\"Preparing the COVID dataset for training...\")\n",
    "    X_covid, y_covid = generate_features_labels(df_covid)\n",
    "    X_covid_scaled = X_covid#standardize_features(X_covid)\n",
    "\n",
    "    print(\"Preparing the Pre-COVID dataset for testing...\")\n",
    "    X_pre_covid, y_pre_covid = generate_features_labels(df_pre_covid)\n",
    "    _, X_pre_covid_scaled = X_covid, X_pre_covid  # Ensure both datasets are scaled using the COVID scaler\n",
    "\n",
    "    print(\"Training the model on COVID data...\")\n",
    "    clf_covid = train_model(X_covid_scaled, y_covid)\n",
    "\n",
    "    print(\"Testing the model on Pre-COVID data...\")\n",
    "    evaluate_model(clf_covid, X_pre_covid_scaled, y_pre_covid)\n",
    "\n",
    "    print(\"Training the model on Pre-COVID data...\")\n",
    "    clf_pre_covid = train_model(X_pre_covid_scaled, y_pre_covid)\n",
    "\n",
    "    print(\"Testing the model on COVID data...\")\n",
    "    evaluate_model(clf_pre_covid, X_covid_scaled, y_covid)\n",
    "    \n",
    "    print(\"\\nCOVID dataset evaluation:\")\n",
    "    prepare_train_test_evaluate(df_covid)\n",
    "\n",
    "    print(\"\\nPre-COVID dataset evaluation:\")\n",
    "    prepare_train_test_evaluate(df_pre_covid)\n",
    "\n",
    "    # Additional scenarios follow similar structure\n",
    "# pre-COVID dataset for both training and testing\n",
    "# COVID dataset for both training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "Training the model on COVID data and testing on Pre-COVID data...\n",
      "Training the model...\n",
      "Testing the model...\n",
      "RMSE: 31.18\n",
      "R^2: 0.10\n",
      "MAPE: 67.48%\n",
      "Training the model...\n",
      "Testing the model...\n",
      "RMSE: 29.73\n",
      "R^2: 0.12\n",
      "MAPE: 65.55%\n",
      "Training the model on Pre-COVID data and testing on COVID data...\n",
      "Training the model...\n",
      "Testing the model...\n",
      "RMSE: 29.73\n",
      "R^2: 0.12\n",
      "MAPE: 65.55%\n",
      "Training the model...\n",
      "Testing the model...\n",
      "RMSE: 31.18\n",
      "R^2: 0.10\n",
      "MAPE: 67.48%\n",
      "\n",
      "COVID dataset evaluation:\n",
      "Training the model...\n",
      "Testing the model...\n",
      "RMSE: 31.18\n",
      "R^2: 0.10\n",
      "MAPE: 67.48%\n",
      "\n",
      "Pre-COVID dataset evaluation:\n",
      "Training the model...\n",
      "Testing the model...\n",
      "RMSE: 29.73\n",
      "R^2: 0.12\n",
      "MAPE: 65.55%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "def preprocess_df(df):\n",
    "    quantile_001 = df['duration'].quantile(0.02)\n",
    "    quantile_099 = df['duration'].quantile(0.98)\n",
    "    df_filtered = df[(df['duration'] >= quantile_001) & (df['duration'] <= quantile_099) & (df['duration'] > 5)]\n",
    "    return df_filtered.dropna().sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def load_and_preprocess(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return preprocess_df(df)\n",
    "\n",
    "def generate_features_labels(df):\n",
    "    X = df.drop(['duration'], axis=1)\n",
    "    y = np.log1p(df['duration'])\n",
    "    return X, y\n",
    "\n",
    "def train_model(X_train, y_train, model=lgb.LGBMRegressor(n_estimators=300,random_state=42)):#XGBRegressor(n_estimators=200, n_jobs=10, objective='reg:squarederror')):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_test=np.expm1(y_test)\n",
    "    y_pred = np.expm1(model.predict(X_test))\n",
    "    \n",
    "    # Calculating Mean Squared Error (MSE) and R^2 Score\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculating Mean Absolute Percentage Error (MAPE)\n",
    "    # Avoid division by zero by replacing 0 with a very small number (np.finfo(float).eps)\n",
    "    y_test_safe = np.where(y_test == 0, np.finfo(float).eps, y_test)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test_safe)) * 100\n",
    "    \n",
    "    # Printing the evaluation metrics\n",
    "    print(f'RMSE: {np.sqrt(mse):.2f}')\n",
    "    print(f'R^2: {r2:.2f}')\n",
    "    print(f'MAPE: {mape:.2f}%')\n",
    "\n",
    "def prepare_train_test_evaluate(df, test_size=0.2, random_state=420):\n",
    "    X, y = generate_features_labels(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    clf = train_model(X_train_scaled, y_train)\n",
    "\n",
    "    print(\"Testing the model...\")\n",
    "    evaluate_model(clf, X_test_scaled, y_test)\n",
    "    \n",
    "    \n",
    "# Main workflow\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading the datasets...\")\n",
    "    df_covid = load_and_preprocess('merged_covid.csv')\n",
    "    df_pre_covid = load_and_preprocess('merged_before.csv')\n",
    "\n",
    "    # Training on COVID data and testing on Pre-COVID data\n",
    "    print(\"Training the model on COVID data and testing on Pre-COVID data...\")\n",
    "    prepare_train_test_evaluate(df_covid)\n",
    "    prepare_train_test_evaluate(df_pre_covid)\n",
    "\n",
    "    # Training on Pre-COVID data and testing on COVID data\n",
    "    print(\"Training the model on Pre-COVID data and testing on COVID data...\")\n",
    "    prepare_train_test_evaluate(df_pre_covid)\n",
    "    prepare_train_test_evaluate(df_covid)\n",
    "\n",
    "    # COVID dataset for both training and testing\n",
    "    print(\"\\nCOVID dataset evaluation:\")\n",
    "    prepare_train_test_evaluate(df_covid)\n",
    "\n",
    "    # Pre-COVID dataset for both training and testing\n",
    "    print(\"\\nPre-COVID dataset evaluation:\")\n",
    "    prepare_train_test_evaluate(df_pre_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "Preparing the COVID dataset for training...\n",
      "Preparing the Pre-COVID dataset for testing...\n",
      "Standardizing the features...\n",
      "Training the model on COVID data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python3\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model on Pre-COVID data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [49688, 28279]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting the model on Pre-COVID data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m y_pred_pre_covid \u001b[38;5;241m=\u001b[39m clf_covid\u001b[38;5;241m.\u001b[39mpredict(X_pre_covid_scaled)\n\u001b[1;32m---> 49\u001b[0m accuracy_pre_covid \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pre_covid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_pre_covid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy on Pre-COVID data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_pre_covid\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_pre_covid, y_pred_pre_covid))\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m validate_parameter_constraints(\n\u001b[0;32m    188\u001b[0m     parameter_constraints, params, caller_name\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[0;32m    189\u001b[0m )\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    202\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:221\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [49688, 28279]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def preprocess_df(df):\n",
    "    quantile_001 = df['duration'].quantile(0.02)\n",
    "    quantile_099 = df['duration'].quantile(0.98)\n",
    "    df_filtered = df[(df['duration'] >= quantile_001) & (df['duration'] <= quantile_099) & (df['duration'] > 3)]\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "THRESHOLD=30\n",
    "\n",
    "print(\"Loading the datasets...\")\n",
    "df_covid = pd.read_csv('merged_covid.csv')\n",
    "df_pre_covid = pd.read_csv('merged_before.csv')\n",
    "\n",
    "df_covid = preprocess_df(df_covid).dropna()\n",
    "df_pre_covid = preprocess_df(df_pre_covid).dropna()\n",
    "\n",
    "# quantile_001 = df['duration'].quantile(0.02)\n",
    "# quantile_099 = df['duration'].quantile(0.98)\n",
    "# df = df[(df['duration'] >= quantile_001) & (df['duration'] <= quantile_099) & (df['duration']>3)]\n",
    "\n",
    "\n",
    "# features = ['Main_Category', 'Day', 'Is_Major_Incident', 'Num_Vehicles_Involved', 'Month', 'Hour', 'distance_to_CBD']\n",
    "print(\"Preparing the COVID dataset for training...\")\n",
    "X_covid = df_covid.drop(['duration'],axis=1)#[features]\n",
    "y_covid = (df_covid['duration'] > THRESHOLD).astype(int)  # Binary classification\n",
    "\n",
    "print(\"Preparing the Pre-COVID dataset for testing...\")\n",
    "X_pre_covid = df_pre_covid.drop(['duration'],axis=1)#[features]\n",
    "y_pre_covid = (df_pre_covid['duration'] > THRESHOLD).astype(int)\n",
    "\n",
    "print(\"Standardizing the features...\")\n",
    "scaler = StandardScaler()\n",
    "X_covid_scaled = scaler.fit_transform(X_covid)\n",
    "X_pre_covid_scaled = scaler.transform(X_pre_covid)  # Use the same scaler for consistency\n",
    "\n",
    "print(\"Training the model on COVID data...\")\n",
    "clf_covid = XGBClassifier(use_label_encoder=False)#RandomForestClassifier(random_state=42)\n",
    "clf_covid.fit(X_covid_scaled, y_covid)\n",
    "\n",
    "print(\"Testing the model on Pre-COVID data...\")\n",
    "y_pred_pre_covid = clf_covid.predict(X_pre_covid_scaled)\n",
    "accuracy_pre_covid = accuracy_score(y_pre_covid, y_pred_pre_covid)\n",
    "print(f'Accuracy on Pre-COVID data: {accuracy_pre_covid:.2f}')\n",
    "print(classification_report(y_pre_covid, y_pred_pre_covid))\n",
    "# ```\n",
    "\n",
    "### Scenario 2: Train on Pre-COVID data, Test on COVID data\n",
    "\n",
    "# ```python\n",
    "print(\"Training the model on Pre-COVID data...\")\n",
    "clf_pre_covid = RandomForestClassifier(random_state=42)\n",
    "clf_pre_covid.fit(X_pre_covid_scaled, y_pre_covid)\n",
    "\n",
    "print(\"Testing the model on COVID data...\")\n",
    "y_pred_covid = clf_pre_covid.predict(X_covid_scaled)\n",
    "accuracy_covid = accuracy_score(y_covid, y_pred_covid)\n",
    "print(f'Accuracy on COVID data: {accuracy_covid:.2f}')\n",
    "print(classification_report(y_covid, y_pred_covid))\n",
    "# ```\n",
    "\n",
    "### Scenario 3: Evaluate a model trained on merged and cleaned COVID and Pre-COVID data\n",
    "\n",
    "# ```python\n",
    "print(\"Merging the datasets for a comprehensive analysis...\")\n",
    "df_merged = pd.concat([df_covid, df_pre_covid])\n",
    "\n",
    "print(\"Cleaning the merged dataset...\")\n",
    "df_merged_clean = df_merged.dropna(subset=['Main_Category', 'Day', 'Num_Vehicles_Involved', 'Month', 'Hour', 'distance_to_CBD', 'duration'])\n",
    "\n",
    "features = ['Main_Category', 'Day', 'Num_Vehicles_Involved', 'Month', 'Hour', 'distance_to_CBD']\n",
    "print(\"Preparing the data after cleaning...\")\n",
    "X = df_merged_clean[features]\n",
    "y = (df_merged_clean['duration'] > THRESHOLD).astype(int)\n",
    "\n",
    "print(\"Standardizing the features...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Splitting the data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Testing the model...\")\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Preparing the Pre-COVID dataset for both training and testing...\")\n",
    "X_pre_covid_only = df_pre_covid[features]\n",
    "y_pre_covid_only = (df_pre_covid['duration'] > THRESHOLD).astype(int)\n",
    "\n",
    "print(\"Standardizing the Pre-COVID features for internal evaluation...\")\n",
    "X_pre_covid_only_scaled = scaler.fit_transform(X_pre_covid_only)\n",
    "\n",
    "print(\"Splitting the Pre-COVID data into training and testing sets...\")\n",
    "X_pre_train, X_pre_test, y_pre_train, y_pre_test = train_test_split(X_pre_covid_only_scaled, y_pre_covid_only, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training the model on Pre-COVID data (internal evaluation)...\")\n",
    "clf_pre_internal = XGBClassifier(use_label_encoder=False)#RandomForestClassifier(random_state=42)\n",
    "clf_pre_internal.fit(X_pre_train, y_pre_train)\n",
    "\n",
    "print(\"Testing the model on Pre-COVID data (internal evaluation)...\")\n",
    "y_pre_pred_internal = clf_pre_internal.predict(X_pre_test)\n",
    "accuracy_pre_internal = accuracy_score(y_pre_test, y_pre_pred_internal)\n",
    "print(f'Internal Accuracy on Pre-COVID data: {accuracy_pre_internal:.2f}')\n",
    "print(classification_report(y_pre_test, y_pre_pred_internal))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Preparing the COVID dataset for both training and testing...\")\n",
    "X_covid_only = df_covid[features]\n",
    "y_covid_only = (df_covid['duration'] > THRESHOLD).astype(int)\n",
    "\n",
    "print(\"Standardizing the COVID features for internal evaluation...\")\n",
    "X_covid_only_scaled = scaler.fit_transform(X_covid_only)\n",
    "\n",
    "print(\"Splitting the COVID data into training and testing sets...\")\n",
    "X_covid_train, X_covid_test, y_covid_train, y_covid_test = train_test_split(X_covid_only_scaled, y_covid_only, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training the model on COVID data (internal evaluation)...\")\n",
    "clf_covid_internal =XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "clf_covid_internal.fit(X_covid_train, y_covid_train)\n",
    "\n",
    "print(\"Testing the model on COVID data (internal evaluation)...\")\n",
    "y_covid_pred_internal = clf_covid_internal.predict(X_covid_test)\n",
    "accuracy_covid_internal = accuracy_score(y_covid_test, y_covid_pred_internal)\n",
    "print(f'Internal Accuracy on COVID data: {accuracy_covid_internal:.2f}')\n",
    "print(classification_report(y_covid_test, y_covid_pred_internal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "Loading the datasets...\n",
      "Preparing the COVID dataset for training...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Is_Major_Incident'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMain_Category\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIs_Major_Incident\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNum_Vehicles_Involved\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_to_CBD\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing the COVID dataset for training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m X_covid \u001b[38;5;241m=\u001b[39m \u001b[43mdf_covid\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     38\u001b[0m y_covid \u001b[38;5;241m=\u001b[39m (df_covid[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m THRESHOLD)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)  \u001b[38;5;66;03m# Binary classification\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing the Pre-COVID dataset for testing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\pandas\\core\\frame.py:3766\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3764\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3765\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3766\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3768\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5876\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5874\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5876\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5878\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5880\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5937\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 5938\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Is_Major_Incident'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "THRESHOLD = 40\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def preprocess_df(df):\n",
    "    quantile_001 = df['duration'].quantile(0.02)\n",
    "    quantile_099 = df['duration'].quantile(0.98)\n",
    "    df_filtered = df[(df['duration'] >= quantile_001) & (df['duration'] <= quantile_099) & (df['duration'] > 3)]\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "THRESHOLD=30\n",
    "\n",
    "print(\"Loading the datasets...\")\n",
    "df_covid = pd.read_csv('merged_covid.csv')\n",
    "df_pre_covid = pd.read_csv('merged_before.csv')\n",
    "\n",
    "df_covid = preprocess_df(df_covid)\n",
    "df_pre_covid = preprocess_df(df_pre_covid)\n",
    "\n",
    "\n",
    "print(\"Loading the datasets...\")\n",
    "# df_covid = pd.read_csv('merged_covid.csv')\n",
    "# df_pre_covid = pd.read_csv('merged_before.csv')\n",
    "\n",
    "features = ['Main_Category', 'Day', 'Is_Major_Incident', 'Num_Vehicles_Involved', 'Month', 'Hour', 'distance_to_CBD']\n",
    "print(\"Preparing the COVID dataset for training...\")\n",
    "X_covid = df_covid[features]\n",
    "y_covid = (df_covid['duration'] > THRESHOLD).astype(int)  # Binary classification\n",
    "\n",
    "print(\"Preparing the Pre-COVID dataset for testing...\")\n",
    "X_pre_covid = df_pre_covid[features]\n",
    "y_pre_covid = (df_pre_covid['duration'] > THRESHOLD).astype(int)\n",
    "\n",
    "print(\"Training the model on COVID data...\")\n",
    "clf_covid = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "clf_covid.fit(X_covid, y_covid)\n",
    "\n",
    "print(\"Testing the model on Pre-COVID data...\")\n",
    "y_pred_pre_covid = clf_covid.predict(X_pre_covid)\n",
    "accuracy_pre_covid = accuracy_score(y_pre_covid, y_pred_pre_covid)\n",
    "print(f'Accuracy on Pre-COVID data: {accuracy_pre_covid:.2f}')\n",
    "print(classification_report(y_pre_covid, y_pred_pre_covid))\n",
    "\n",
    "# Scenario 2\n",
    "print(\"Training the model on Pre-COVID data...\")\n",
    "clf_pre_covid = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "clf_pre_covid.fit(X_pre_covid, y_pre_covid)\n",
    "\n",
    "print(\"Testing the model on COVID data...\")\n",
    "y_pred_covid = clf_pre_covid.predict(X_covid)\n",
    "accuracy_covid = accuracy_score(y_covid, y_pred_covid)\n",
    "print(f'Accuracy on COVID data: {accuracy_covid:.2f}')\n",
    "print(classification_report(y_covid, y_pred_covid))\n",
    "\n",
    "# Scenario 3\n",
    "print(\"Merging the datasets for a comprehensive analysis...\")\n",
    "df_merged = pd.concat([df_covid, df_pre_covid])\n",
    "\n",
    "print(\"Cleaning the merged dataset...\")\n",
    "df_merged_clean = df_merged.dropna(subset=features + ['duration'])\n",
    "\n",
    "print(\"Preparing the data after cleaning...\")\n",
    "X = df_merged_clean[features]\n",
    "y = (df_merged_clean['duration'] > THRESHOLD).astype(int)\n",
    "\n",
    "print(\"Splitting the data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Testing the model...\")\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "### Below we perform internal evaluations for Pre-COVID and COVID datasets in isolation\n",
    "## Pre-COVID Dataset Internal Evaluation\n",
    "print(\"Splitting the Pre-COVID data into training and testing sets (internal)...\")\n",
    "X_pre_train, X_pre_test, y_pre_train, y_pre_test = train_test_split(X_pre_covid, y_pre_covid, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training the model on Pre-COVID data (internal)...\")\n",
    "clf_pre_internal = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "clf_pre_internal.fit(X_pre_train, y_pre_train)\n",
    "\n",
    "print(\"Testing the model on Pre-COVID data (internal)...\")\n",
    "y_pre_pred_internal = clf_pre_internal.predict(X_pre_test)\n",
    "accuracy_pre_internal = accuracy_score(y_pre_test, y_pre_pred_internal)\n",
    "print(f'Internal Accuracy on Pre-COVID data: {accuracy_pre_internal:.2f}')\n",
    "print(classification_report(y_pre_test, y_pre_pred_internal))\n",
    "\n",
    "## COVID Dataset Internal Evaluation\n",
    "print(\"Splitting the COVID data into training and testing sets (internal)...\")\n",
    "X_covid_train, X_covid_test, y_covid_train, y_covid_test = train_test_split(X_covid, y_covid, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training the model on COVID data (internal)...\")\n",
    "clf_covid_internal = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "clf_covid_internal.fit(X_covid_train, y_covid_train)\n",
    "\n",
    "print(\"Testing the model on COVID data (internal)...\")\n",
    "y_covid_pred_internal = clf_covid_internal.predict(X_covid_test)\n",
    "accuracy_covid_internal = accuracy_score(y_covid_test, y_covid_pred_internal)\n",
    "print(f'Internal Accuracy on COVID data: {accuracy_covid_internal:.2f}')\n",
    "print(classification_report(y_covid_test, y_covid_pred_internal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Preparing the Pre-COVID dataset for both training and testing...\")\n",
    "# X_pre_covid_only = df_pre_covid[features]\n",
    "# y_pre_covid_only = (df_pre_covid['duration'] > THRESHOLD).astype(int)\n",
    "\n",
    "# print(\"Standardizing the Pre-COVID features for internal evaluation...\")\n",
    "# X_pre_covid_only_scaled = scaler.fit_transform(X_pre_covid_only)\n",
    "\n",
    "# print(\"Splitting the Pre-COVID data into training and testing sets...\")\n",
    "# X_pre_train, X_pre_test, y_pre_train, y_pre_test = train_test_split(X_pre_covid_only_scaled, y_pre_covid_only, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(\"Training the model on Pre-COVID data (internal evaluation)...\")\n",
    "# clf_pre_internal = RandomForestClassifier(random_state=42)\n",
    "# clf_pre_internal.fit(X_pre_train, y_pre_train)\n",
    "\n",
    "# print(\"Testing the model on Pre-COVID data (internal evaluation)...\")\n",
    "# y_pre_pred_internal = clf_pre_internal.predict(X_pre_test)\n",
    "# accuracy_pre_internal = accuracy_score(y_pre_test, y_pre_pred_internal)\n",
    "# print(f'Internal Accuracy on Pre-COVID data: {accuracy_pre_internal:.2f}')\n",
    "# print(classification_report(y_pre_test, y_pre_pred_internal))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Preparing the COVID dataset for both training and testing...\")\n",
    "# X_covid_only = df_covid[features]\n",
    "# y_covid_only = (df_covid['duration'] > THRESHOLD).astype(int)\n",
    "\n",
    "# print(\"Standardizing the COVID features for internal evaluation...\")\n",
    "# X_covid_only_scaled = scaler.fit_transform(X_covid_only)\n",
    "\n",
    "# print(\"Splitting the COVID data into training and testing sets...\")\n",
    "# X_covid_train, X_covid_test, y_covid_train, y_covid_test = train_test_split(X_covid_only_scaled, y_covid_only, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(\"Training the model on COVID data (internal evaluation)...\")\n",
    "# clf_covid_internal = RandomForestClassifier(random_state=42)\n",
    "# clf_covid_internal.fit(X_covid_train, y_covid_train)\n",
    "\n",
    "# print(\"Testing the model on COVID data (internal evaluation)...\")\n",
    "# y_covid_pred_internal = clf_covid_internal.predict(X_covid_test)\n",
    "# accuracy_covid_internal = accuracy_score(y_covid_test, y_covid_pred_internal)\n",
    "# print(f'Internal Accuracy on COVID data: {accuracy_covid_internal:.2f}')\n",
    "# print(classification_report(y_covid_test, y_covid_pred_internal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "Standardizing the features...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Is_Major_Incident'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStandardizing the features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 30\u001b[0m X_covid_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdf_covid\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     31\u001b[0m X_pre_covid_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(df_pre_covid[features])  \u001b[38;5;66;03m# Use the same scaler for consistency\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Scenario 1: Train on COVID data, Test on Pre-COVID data\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\pandas\\core\\frame.py:3766\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3764\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3765\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3766\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3768\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5876\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5874\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5876\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5878\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5880\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5937\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 5938\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Is_Major_Incident'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "THRESHOLD = 30\n",
    "\n",
    "print(\"Loading the datasets...\")\n",
    "df_covid = pd.read_csv('merged_covid.csv')\n",
    "df_pre_covid = pd.read_csv('merged_before.csv')\n",
    "\n",
    "quantile_001 = pd.concat([df_covid, df_pre_covid])['duration'].quantile(0.01)\n",
    "quantile_099 = pd.concat([df_covid, df_pre_covid])['duration'].quantile(0.99)\n",
    "df_covid = df_covid[(df_covid['duration'] >= quantile_001) & (df_covid['duration'] <= quantile_099)]\n",
    "df_pre_covid = df_pre_covid[(df_pre_covid['duration'] >= quantile_001) & (df_pre_covid['duration'] <= quantile_099)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "features = ['Main_Category', 'Day', 'Is_Major_Incident', 'Num_Vehicles_Involved', 'Month', 'Hour', 'distance_to_CBD']\n",
    "\n",
    "# For regression, we directly use 'duration' without transforming it into binary categories\n",
    "y_covid = df_covid['duration']\n",
    "y_pre_covid = df_pre_covid['duration']\n",
    "\n",
    "print(\"Standardizing the features...\")\n",
    "scaler = StandardScaler()\n",
    "X_covid_scaled = scaler.fit_transform(df_covid[features])\n",
    "X_pre_covid_scaled = scaler.transform(df_pre_covid[features])  # Use the same scaler for consistency\n",
    "\n",
    "# Scenario 1: Train on COVID data, Test on Pre-COVID data\n",
    "print(\"Training the model on COVID data for regression...\")\n",
    "regressor_covid = RandomForestRegressor(random_state=42)\n",
    "regressor_covid.fit(X_covid_scaled, y_covid)\n",
    "\n",
    "print(\"Testing the model on Pre-COVID data for regression...\")\n",
    "y_pred_pre_covid = regressor_covid.predict(X_pre_covid_scaled)\n",
    "mse_pre_covid = mean_squared_error(y_pre_covid, y_pred_pre_covid)\n",
    "print(f'RMSE on Pre-COVID data: {np.sqrt(mse_pre_covid):.2f}')\n",
    "print(f'R2 Score on Pre-COVID data: {r2_score(y_pre_covid, y_pred_pre_covid):.2f}')\n",
    "\n",
    "# Scenario 2: Train on Pre-COVID data, Test on COVID data\n",
    "print(\"Training the model on Pre-COVID data for regression...\")\n",
    "regressor_pre_covid = RandomForestRegressor(random_state=42)\n",
    "regressor_pre_covid.fit(X_pre_covid_scaled, y_pre_covid)\n",
    "\n",
    "print(\"Testing the model on COVID data for regression...\")\n",
    "y_pred_covid = regressor_pre_covid.predict(X_covid_scaled)\n",
    "mse_covid = mean_squared_error(y_covid, y_pred_covid)\n",
    "print(f'RMSE on COVID data: {np.sqrt(mse_covid):.2f}')\n",
    "print(f'R2 Score on COVID data: {r2_score(y_covid, y_pred_covid):.2f}')\n",
    "\n",
    "# Scenario 3: Train and test on merged and cleaned COVID and Pre-COVID data\n",
    "print(\"Merging and cleaning the datasets for a comprehensive analysis...\")\n",
    "df_merged = pd.concat([df_covid, df_pre_covid])\n",
    "df_merged_clean = df_merged.dropna(subset=features + ['duration'])\n",
    "\n",
    "X = df_merged_clean[features]\n",
    "y = df_merged_clean['duration']\n",
    "\n",
    "print(\"Standardizing the features for merged data...\")\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Splitting the merged data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training the regression model on merged data...\")\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Testing the regression model on merged data...\")\n",
    "y_pred = model.predict(X_test)\n",
    "mse_merged = mean_squared_error(y_test, y_pred)\n",
    "print(f'RMSE on Merged data: {np.sqrt(mse_merged):.2f}')\n",
    "print(f'R2 Score on Merged data: {r2_score(y_test, y_pred):.2f}')\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have already loaded and cleaned your datasets as per your previous steps\n",
    "df_covid = pd.read_csv('merged_covid.csv')\n",
    "df_pre_covid = pd.read_csv('merged_before.csv')\n",
    "\n",
    "# Merging datasets \n",
    "df_merged = pd.concat([df_covid, df_pre_covid])\n",
    "\n",
    "# Removing outliers based on the duration column across the whole dataset\n",
    "quantile_001 = df_merged['duration'].quantile(0.01)\n",
    "quantile_099 = df_merged['duration'].quantile(0.99)\n",
    "df_merged = df_merged[(df_merged['duration'] >= quantile_001) & (df_merged['duration'] <= quantile_099)]\n",
    "\n",
    "# Dropping missing values in features and target column\n",
    "df_merged_clean = df_merged.dropna(subset=features + ['duration'])\n",
    "\n",
    "# Features and target variable\n",
    "features = ['Main_Category', 'Day', 'Is_Major_Incident', 'Num_Vehicles_Involved', 'Month', 'Hour', 'distance_to_CBD']\n",
    "X = df_merged_clean[features]\n",
    "y = df_merged_clean['duration']\n",
    "\n",
    "# Standardizing features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting the merged dataset into training & testing sets with random shuffling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # 80% training, 20% testing\n",
    "\n",
    "# Initializing and training the RandomForestRegressor\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating and printing RMSE and R2 Score \n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'RMSE on Merged and randomly sampled data: {np.sqrt(mse):.2f}')\n",
    "print(f'R2 Score on Merged and randomly sampled data: {r2_score(y_test, y_pred):.2f}')\n",
    "\n",
    "print(\"Preparing the Pre-COVID dataset for regression (internal evaluation)...\")\n",
    "X_pre_covid_only_scaled = scaler.fit_transform(df_pre_covid[features])\n",
    "y_pre_covid_only = df_pre_covid['duration']\n",
    "\n",
    "print(\"Splitting the Pre-COVID data into training and testing sets (internal evaluation)...\")\n",
    "X_pre_train, X_pre_test, y_pre_train, y_pre_test = train_test_split(X_pre_covid_only_scaled, y_pre_covid_only, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training the regression model on Pre-COVID data (internal evaluation)...\")\n",
    "regressor_pre_internal = RandomForestRegressor(random_state=42)\n",
    "regressor_pre_internal.fit(X_pre_train, y_pre_train)\n",
    "\n",
    "print(\"Testing the regression model on Pre-COVID data (internal evaluation)...\")\n",
    "y_pre_pred_internal = regressor_pre_internal.predict(X_pre_test)\n",
    "mse_pre_internal = mean_squared_error(y_pre_test, y_pre_pred_internal)\n",
    "print(f'RMSE on Pre-COVID data (internal evaluation): {np.sqrt(mse_pre_internal):.2f}')\n",
    "print(f'R2 Score on Pre-COVID data (internal evaluation): {r2_score(y_pre_test, y_pre_pred_internal):.2f}')\n",
    "\n",
    "\n",
    "print(\"Preparing the COVID dataset for regression (internal evaluation)...\")\n",
    "X_covid_only_scaled = scaler.fit_transform(df_covid[features])\n",
    "y_covid_only = df_covid['duration']\n",
    "\n",
    "print(\"Splitting the COVID data into training and testing sets (internal evaluation)...\")\n",
    "X_covid_train, X_covid_test, y_covid_train, y_covid_test = train_test_split(X_covid_only_scaled, y_covid_only, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training the regression model on COVID data (internal evaluation)...\")\n",
    "regressor_covid_internal = RandomForestRegressor(random_state=42)\n",
    "regressor_covid_internal.fit(X_covid_train, y_covid_train)\n",
    "\n",
    "print(\"Testing the regression model on COVID data (internal evaluation)...\")\n",
    "y_covid_pred_internal = regressor_covid_internal.predict(X_covid_test)\n",
    "mse_covid_internal = mean_squared_error(y_covid_test, y_covid_pred_internal)\n",
    "print(f'RMSE on COVID data (internal evaluation): {np.sqrt(mse_covid_internal):.2f}')\n",
    "print(f'R2 Score on COVID data (internal evaluation): {r2_score(y_covid_test, y_covid_pred_internal):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on Merged data: 38.25\n",
      "R2 Score on Merged data: -0.05\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['duration'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m y_pre_covid \u001b[38;5;241m=\u001b[39m df_pre_covid[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Sort dataframe based on features list to avoid missing value issues if present\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m X_covid \u001b[38;5;241m=\u001b[39m \u001b[43mX_covid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mduration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#.dropna()\u001b[39;00m\n\u001b[0;32m     51\u001b[0m X_pre_covid \u001b[38;5;241m=\u001b[39m X_pre_covid\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#.loc[:, features].dropna()\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# No need for scaling due to XGBoost's handling of raw features\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Scenario 1: Training on COVID, testing on Pre-COVID\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\pandas\\core\\frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5119\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5120\u001b[0m     labels: IndexLabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5127\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5128\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5130\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5131\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5264\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5268\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5272\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5273\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\pandas\\core\\generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4547\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4549\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\pandas\\core\\generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4589\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4591\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4592\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4594\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6696\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6695\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6696\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6697\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['duration'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load your datasets\n",
    "df_covid = pd.read_csv('merged_covid.csv')\n",
    "df_pre_covid = pd.read_csv('merged_before.csv')\n",
    "\n",
    "# Define the relevant features for your models\n",
    "# features = ['Main_Category', 'Day', 'Is_Major_Incident', 'Num_Vehicles_Involved', 'Month', 'Hour', 'distance_to_CBD']\n",
    "\n",
    "# Merging and cleaning datasets for consistent preprocessing across scenarios\n",
    "df_merged = pd.concat([df_covid, df_pre_covid])\n",
    "quantile_001 = df_merged['duration'].quantile(0.01)\n",
    "quantile_099 = df_merged['duration'].quantile(0.99)\n",
    "\n",
    "df_merged = df_merged[(df_merged['duration'] >= quantile_001) & (df_merged['duration'] <= quantile_099)]\n",
    "df_merged_clean = df_merged.dropna()#subset=features + ['duration'])\n",
    "\n",
    "# Split merged_clean for a comprehensive dataset analysis\n",
    "X = df_merged_clean.drop(['duration'],axis=1)#[features]\n",
    "y = df_merged_clean['duration']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the XGBoost regressor on the merged dataset\n",
    "xgb_model_merged = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 10, random_state=42)\n",
    "\n",
    "xgb_model_merged.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = xgb_model_merged.predict(X_test)\n",
    "\n",
    "# Calculating and displaying RMSE and R2 Score for Merged data\n",
    "mse_merged = mean_squared_error(y_test, y_pred)\n",
    "print(f'RMSE on Merged data: {np.sqrt(mse_merged):.2f}')\n",
    "print(f'R2 Score on Merged data: {r2_score(y_test, y_pred):.2f}')\n",
    "\n",
    "# Now, for scenario-specific models, extract and directly apply preprocessing from the merged dataset:\n",
    "X_covid = df_covid.drop(['duration'],axis=1)\n",
    "y_covid = df_covid['duration']\n",
    "X_pre_covid = df_pre_covid.drop(['duration'],axis=1)\n",
    "y_pre_covid = df_pre_covid['duration']\n",
    "\n",
    "# Sort dataframe based on features list to avoid missing value issues if present\n",
    "X_covid = X_covid.drop(['duration'],axis=1)#.dropna()\n",
    "X_pre_covid = X_pre_covid.drop(['duration'],axis=1)#.loc[:, features].dropna()\n",
    "\n",
    "# No need for scaling due to XGBoost's handling of raw features\n",
    "\n",
    "# Scenario 1: Training on COVID, testing on Pre-COVID\n",
    "xgb_model_covid = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 10, random_state=42)\n",
    "\n",
    "xgb_model_covid.fit(X_covid, y_covid)\n",
    "\n",
    "# Prediction on Pre-COVID data\n",
    "y_pred_pre_covid = xgb_model_covid.predict(X_pre_covid)\n",
    "mse_pre_covid = mean_squared_error(y_pre_covid, y_pred_pre_covid)\n",
    "\n",
    "print(f'RMSE on Pre-COVID data (trained on COVID): {np.sqrt(mse_pre_covid):.2f}')\n",
    "print(f'R2 Score on Pre-COVID data (trained on COVID): {r2_score(y_pre_covid, y_pred_pre_covid):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Initializing and training the RandomForestRegressor\u001b[39;00m\n\u001b[0;32m     36\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Making predictions on the test set\u001b[39;00m\n\u001b[0;32m     40\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 184\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\sklearn\\tree\\_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \n\u001b[0;32m   1221\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have already loaded and cleaned your datasets as per your previous steps\n",
    "df_covid = pd.read_csv('merged_covid.csv')\n",
    "df_pre_covid = pd.read_csv('merged_before.csv')\n",
    "\n",
    "# Merging datasets \n",
    "df_merged = pd.concat([df_covid, df_pre_covid])\n",
    "\n",
    "# Removing outliers based on the duration column across the whole dataset\n",
    "quantile_001 = df_merged['duration'].quantile(0.01)\n",
    "quantile_099 = df_merged['duration'].quantile(0.99)\n",
    "df_merged = df_merged[(df_merged['duration'] >= quantile_001) & (df_merged['duration'] <= quantile_099)]\n",
    "\n",
    "# Dropping missing values in features and target column\n",
    "df_merged_clean = df_merged.dropna()#subset=features + ['duration'])\n",
    "\n",
    "# Features and target variable\n",
    "# features = ['Main_Category', 'Day', 'Is_Major_Incident', 'Num_Vehicles_Involved', 'Month', 'Hour', 'distance_to_CBD']\n",
    "X = df_merged_clean.drop(['duration'],axis=1)#[features]\n",
    "y = df_merged_clean['duration']\n",
    "\n",
    "# Standardizing features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting the merged dataset into training & testing sets with random shuffling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # 80% training, 20% testing\n",
    "\n",
    "# Initializing and training the RandomForestRegressor\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating and printing RMSE and R2 Score \n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'RMSE on Merged and randomly sampled data: {np.sqrt(mse):.2f}')\n",
    "print(f'R2 Score on Merged and randomly sampled data: {r2_score(y_test, y_pred):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the Pre-COVID dataset for regression (internal evaluation)...\n",
      "Splitting the Pre-COVID data into training and testing sets (internal evaluation)...\n",
      "Training the regression model on Pre-COVID data (internal evaluation)...\n",
      "Testing the regression model on Pre-COVID data (internal evaluation)...\n",
      "RMSE on Pre-COVID data (internal evaluation): 37.65\n",
      "R2 Score on Pre-COVID data (internal evaluation): -0.05\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing the Pre-COVID dataset for regression (internal evaluation)...\")\n",
    "X_pre_covid_only_scaled = scaler.fit_transform(df_pre_covid[features])\n",
    "y_pre_covid_only = df_pre_covid['duration']\n",
    "\n",
    "print(\"Splitting the Pre-COVID data into training and testing sets (internal evaluation)...\")\n",
    "X_pre_train, X_pre_test, y_pre_train, y_pre_test = train_test_split(X_pre_covid_only_scaled, y_pre_covid_only, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training the regression model on Pre-COVID data (internal evaluation)...\")\n",
    "regressor_pre_internal = RandomForestRegressor(random_state=42)\n",
    "regressor_pre_internal.fit(X_pre_train, y_pre_train)\n",
    "\n",
    "print(\"Testing the regression model on Pre-COVID data (internal evaluation)...\")\n",
    "y_pre_pred_internal = regressor_pre_internal.predict(X_pre_test)\n",
    "mse_pre_internal = mean_squared_error(y_pre_test, y_pre_pred_internal)\n",
    "print(f'RMSE on Pre-COVID data (internal evaluation): {np.sqrt(mse_pre_internal):.2f}')\n",
    "print(f'R2 Score on Pre-COVID data (internal evaluation): {r2_score(y_pre_test, y_pre_pred_internal):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the COVID dataset for regression (internal evaluation)...\n",
      "Splitting the COVID data into training and testing sets (internal evaluation)...\n",
      "Training the regression model on COVID data (internal evaluation)...\n",
      "Testing the regression model on COVID data (internal evaluation)...\n",
      "RMSE on COVID data (internal evaluation): 37.83\n",
      "R2 Score on COVID data (internal evaluation): -0.06\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing the COVID dataset for regression (internal evaluation)...\")\n",
    "X_covid_only_scaled = scaler.fit_transform(df_covid[features])\n",
    "y_covid_only = df_covid['duration']\n",
    "\n",
    "print(\"Splitting the COVID data into training and testing sets (internal evaluation)...\")\n",
    "X_covid_train, X_covid_test, y_covid_train, y_covid_test = train_test_split(X_covid_only_scaled, y_covid_only, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training the regression model on COVID data (internal evaluation)...\")\n",
    "regressor_covid_internal = RandomForestRegressor(random_state=42)\n",
    "regressor_covid_internal.fit(X_covid_train, y_covid_train)\n",
    "\n",
    "print(\"Testing the regression model on COVID data (internal evaluation)...\")\n",
    "y_covid_pred_internal = regressor_covid_internal.predict(X_covid_test)\n",
    "mse_covid_internal = mean_squared_error(y_covid_test, y_covid_pred_internal)\n",
    "print(f'RMSE on COVID data (internal evaluation): {np.sqrt(mse_covid_internal):.2f}')\n",
    "print(f'R2 Score on COVID data (internal evaluation): {r2_score(y_covid_test, y_covid_pred_internal):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
